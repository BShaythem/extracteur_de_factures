{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a49b090",
   "metadata": {},
   "source": [
    "# Donut Model Training Notebook\n",
    "This notebook demonstrates how to train a Donut (Document Understanding Transformer) model for invoice extraction using your labeled dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3399029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if needed)\n",
    "# !pip install transformers datasets torch torchvision seqeval accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b50562",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from transformers import DonutProcessor, VisionEncoderDecoderModel, TrainingArguments, Trainer\n",
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np  \n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a7c5ff",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare the Dataset\n",
    "Assume your images are in `../data/invoices-donut/train` and JSONs in `../data/invoices-donut/donut_json/train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7664447a",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = '../data/invoices-donut/train'\n",
    "json_dir = '../data/invoices-donut/donut_json/train'\n",
    "image_files = sorted([f for f in os.listdir(image_dir) if f.endswith(('.png', '.jpg', '.jpeg'))])\n",
    "data = []\n",
    "for img_file in image_files:\n",
    "    img_path = os.path.join(image_dir, img_file)\n",
    "    json_path = os.path.join(json_dir, os.path.splitext(img_file)[0] + '.json')\n",
    "    if os.path.exists(json_path):\n",
    "        with open(json_path, 'r', encoding='utf-8') as f:\n",
    "            label = json.load(f)\n",
    "        data.append({'image_path': img_path, 'label': label})\n",
    "print(f'Loaded {len(data)} image-label pairs.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb80670",
   "metadata": {},
   "source": [
    "## 2. Visualize a Sample\n",
    "Let's visualize a random sample from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7145ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = random.choice(data)\n",
    "img = Image.open(sample['image_path'])\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.title('Sample Invoice Image')\n",
    "plt.show()\n",
    "print('Label:', json.dumps(sample['label'], indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3989fd7",
   "metadata": {},
   "source": [
    "## 3. Load Donut Processor and Model\n",
    "You can use a pre-trained Donut model from HuggingFace and fine-tune it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7d387f",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = DonutProcessor.from_pretrained('naver-clova-ix/donut-base-finetuned-docvqa')\n",
    "model = VisionEncoderDecoderModel.from_pretrained('naver-clova-ix/donut-base-finetuned-docvqa')\n",
    "model.config.decoder_start_token_id = processor.tokenizer.cls_token_id\n",
    "model.config.pad_token_id = processor.tokenizer.pad_token_id\n",
    "model.config.vocab_size = model.config.decoder.vocab_size\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e97dd60",
   "metadata": {},
   "source": [
    "## 4. Prepare Dataset for Training\n",
    "We need to convert images and labels into the format expected by Donut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e78aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(example):\n",
    "    image = Image.open(example['image_path']).convert('RGB')\n",
    "    pixel_values = processor(image, return_tensors='pt').pixel_values[0]\n",
    "    label_str = json.dumps(example['label'], ensure_ascii=False)\n",
    "    labels = processor.tokenizer(label_str, add_special_tokens=True, max_length=512, padding='max_length', truncation=True, return_tensors='pt').input_ids[0]\n",
    "    return {'pixel_values': pixel_values, 'labels': labels}\n",
    "\n",
    "dataset = Dataset.from_list(data)\n",
    "dataset = dataset.map(preprocess)\n",
    "dataset.set_format(type='torch', columns=['pixel_values', 'labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4956119",
   "metadata": {},
   "source": [
    "## 5. Train the Model\n",
    "Set up training arguments and start training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed72e6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./donut-finetuned-invoice',\n",
    "    per_device_train_batch_size=2,\n",
    "    num_train_epochs=5,\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=10,\n",
    "    learning_rate=5e-5,\n",
    "    fp16=True,\n",
    "    report_to='none'\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    data_collator=None\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf570de",
   "metadata": {},
   "source": [
    "## 6. Save the Fine-tuned Model\n",
    "Save your model and processor for later inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118ac0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('./donut-finetuned-invoice')\n",
    "processor.save_pretrained('./donut-finetuned-invoice')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
